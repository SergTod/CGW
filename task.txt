1) Add halting outputs to the model (if not already present)
A. In your CGW step loop, capture halt_logit every step
Inside CGWModel.forward(...), where you already compute the specialists:
# inside the recurrent loop over reasoning steps t=0..max_steps-1
delta_sup, halt_logit = self.sp_sup(S)     # halt_logit: (B,) or (B,1)

# store per-step halt logits for eval
halt_logits.append(halt_logit.squeeze(-1))  # list of (B,)
After the loop:
halt_logits = torch.stack(halt_logits, dim=1)  # (B, steps)
Return it with logits + stats:
return final_logits, stats, halt_logits
If you already return stats as a dict, you can also store it in stats:
stats["halt_logits"] = halt_logits.detach()
2) Implement early-stop logic in evaluation (only)
A. Add a helper to compute “steps_used” per sample
Put this in your eval_phase0.py (or wherever evaluation happens):
import torch

def compute_steps_used(
    halt_logits: torch.Tensor,  # (B, steps)
    min_steps: int,
    halt_threshold: float
) -> torch.Tensor:
    """
    Returns steps_used for each sample in batch (B,).
    If never halts, steps_used = steps (max_steps).
    """
    B, steps = halt_logits.shape
    halt_prob = torch.sigmoid(halt_logits)  # (B, steps)

    # Allowed-to-halt mask: only after min_steps-1 index
    allowed = torch.zeros_like(halt_prob, dtype=torch.bool)
    allowed[:, min_steps-1:] = True

    # Condition to halt at step t
    will_halt = (halt_prob > halt_threshold) & allowed  # (B, steps)

    # Find first True along time dimension
    # If none True, set to steps-1 then add 1 => steps_used=steps
    idx = torch.argmax(will_halt.int(), dim=1)  # returns 0 even if all False
    has_any = will_halt.any(dim=1)

    # If no halt, override idx to last step
    idx = torch.where(has_any, idx, torch.full_like(idx, steps - 1))

    # steps_used is 1-indexed (e.g., idx=0 -> 1 step used)
    steps_used = idx + 1
    return steps_used
B. Use it in eval, and log variance
In eval, after forward pass:
with torch.no_grad():
    logits, stats, halt_logits = model(x, tau=1.0)  # halt_logits: (B, steps)

steps_used = compute_steps_used(
    halt_logits=halt_logits,
    min_steps=2,              # choose 1–3; start with 2
    halt_threshold=0.7        # start with 0.7; adjust later
)

# Log mean/std and store per-sample if you want
steps_mean = steps_used.float().mean().item()
steps_std  = steps_used.float().std(unbiased=False).item()
print(f"Adaptive steps_used: mean={steps_mean:.2f}, std={steps_std:.2f}")
This alone will make your “trajectory std/mean” non-zero if the sup head ever crosses threshold for some samples.
3) Make the model actually stop “computation” in eval (optional, slightly more work)
The above measures adaptation, but it still runs all steps to produce halt_logits. If you want true compute saving in eval, implement an early-break inside the model loop only when self.training == False.
Inside your model loop:
# after computing halt_logit at step t
if (not self.training) and (t + 1) >= min_steps:
    halt_prob_t = torch.sigmoid(halt_logit.squeeze(-1))  # (B,)
    # If ALL samples in batch are above threshold, you can break safely
    if torch.all(halt_prob_t > halt_threshold):
        break
Important note: breaking only when all samples halt keeps tensor shapes simple.
If you want per-sample early stop (different samples halt at different times), you need masking logic to “freeze” halted samples while continuing others. That is doable, but not necessary for Phase 0.5.
4) Where to put the new config flags
Add to your eval CLI:
--min-steps default 2
--halt-threshold default 0.7
--adaptive-eval action flag
Then:
if args.adaptive_eval:
    steps_used = compute_steps_used(...)
else:
    steps_used = torch.full((x.size(0),), fill_value=max_steps, device=x.device)
5) Sanity check (what you should see)
After you add this, re-run eval:
Some batches should show std > 0
Your trajectory std/mean should become non-zero
Accuracy should remain the same or slightly change (if you actually break early)
If std == 0 always, then your sup head never crosses threshold. In that case:
Lower halt_threshold from 0.7 → 0.6 → 0.5
Or increase training signal for sup (later; do not add complexity now)